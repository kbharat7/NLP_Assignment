{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/Natural-Language-Processing-YU/M3_Assignment/blob/main/scripts/m3_assignment_part_III.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part III\n",
        "Using the previous two tutorials, please answer the following using an encorder-decoder approach and an LSTM compared approach. \n",
        "\n",
        "Please create a transformer-based classifier for English name classification into male or female.\n",
        "\n",
        "There are several datasets for name for male or female classification. In subseuqent iterations, this could be expanded to included more classifications. \n",
        "\n",
        "Below is the source from NLTK, which only has male and female available but could be used for the purposes of this assignment. \n",
        "\n",
        "```\n",
        "names = nltk.corpus.names\n",
        "names.fileids()\n",
        "['female.txt', 'male.txt']\n",
        "male_names = names.words('male.txt')\n",
        "female_names = names.words('female.txt')\n",
        "[w for w in male_names if w in female_names]\n",
        "['Abbey', 'Abbie', 'Abby', 'Addie', 'Adrian', 'Adrien', 'Ajay', 'Alex', 'Alexis',\n",
        "'Alfie', 'Ali', 'Alix', 'Allie', 'Allyn', 'Andie', 'Andrea', 'Andy', 'Angel',\n",
        "'Angie', 'Ariel', 'Ashley', 'Aubrey', 'Augustine', 'Austin', 'Averil', ...]\n",
        "```"
      ],
      "metadata": {
        "id": "QD5ia2HsZpsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Data Preparation:** Download and preprocess a dataset of names, labeling them by gender.\n",
        "2. **Model Training:** Fine-tune a pre-trained DistilBERT model on the labeled names dataset for the task of gender classification.\n",
        "3. **Evaluation:** Configure training arguments, execute the training process, and compute metrics such as accuracy, precision, recall, and F1-score to evaluate the model's performance.\n",
        "4. **Results Interpretation:** Analyze the training outcomes using the computed metrics and log outputs.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import nltk\n",
        "from nltk.corpus import names\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (DistilBertTokenizerFast, DistilBertForSequenceClassification, \n",
        "                          Trainer, TrainingArguments)\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('names')\n",
        "\n",
        "def load_and_prepare_data():\n",
        "    \"\"\"Load names data from NLTK and prepare labels.\"\"\"\n",
        "    male_names = [(name, 0) for name in names.words('male.txt')]\n",
        "    female_names = [(name, 1) for name in names.words('female.txt')]\n",
        "    all_names = male_names + female_names\n",
        "    random.shuffle(all_names)\n",
        "    return zip(*all_names)  # Unzips into two lists\n",
        "\n",
        "def create_datasets(tokenizer, names_train, names_test, labels_train, labels_test):\n",
        "    \"\"\"Tokenize names and create custom dataset objects.\"\"\"\n",
        "    train_encodings = tokenizer(list(names_train), truncation=True, padding=True)\n",
        "    test_encodings = tokenizer(list(names_test), truncation=True, padding=True)\n",
        "    train_dataset = NameDataset(train_encodings, labels_train)\n",
        "    test_dataset = NameDataset(test_encodings, labels_test)\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "class NameDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Custom PyTorch dataset for name classification.\"\"\"\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def setup_trainer(tokenizer):\n",
        "    \"\"\"Initialize tokenizer, model, and training arguments.\"\"\"\n",
        "    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',         \n",
        "        num_train_epochs=3,             \n",
        "        per_device_train_batch_size=32, \n",
        "        per_device_eval_batch_size=64,  \n",
        "        warmup_steps=100,               \n",
        "        weight_decay=0.01,              \n",
        "        logging_dir='./logs',           \n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"epoch\",    \n",
        "        save_strategy=\"epoch\"           \n",
        "    )\n",
        "    return model, training_args\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    \"\"\"Calculate metrics for evaluating the model.\"\"\"\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
        "\n",
        "# Main execution\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "names_list, labels = load_and_prepare_data()\n",
        "names_train, names_test, labels_train, labels_test = train_test_split(names_list, labels, test_size=0.2)\n",
        "train_dataset, test_dataset = create_datasets(tokenizer, names_train, names_test, labels_train, labels_test)\n",
        "model, training_args = setup_trainer(tokenizer)\n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset,\n",
        "                  eval_dataset=test_dataset, compute_metrics=compute_metrics)\n",
        "trainer.train()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[nltk_data] Downloading package names to /home/azureuser/nltk_data...\n[nltk_data]   Package names is already up-to-date!\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/anaconda/envs/myenv/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='597' max='597' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [597/597 00:27, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.387200</td>\n      <td>0.302647</td>\n      <td>0.876023</td>\n      <td>0.902620</td>\n      <td>0.894221</td>\n      <td>0.911178</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.144800</td>\n      <td>0.361893</td>\n      <td>0.874764</td>\n      <td>0.901534</td>\n      <td>0.894014</td>\n      <td>0.909182</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.161600</td>\n      <td>0.365745</td>\n      <td>0.870359</td>\n      <td>0.896794</td>\n      <td>0.900402</td>\n      <td>0.893214</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Attempted to log scalar metric loss:\n0.6973\nAttempted to log scalar metric grad_norm:\n0.726940929889679\nAttempted to log scalar metric learning_rate:\n5e-06\nAttempted to log scalar metric epoch:\n0.05\nAttempted to log scalar metric loss:\n0.6764\nAttempted to log scalar metric grad_norm:\n0.8579685688018799\nAttempted to log scalar metric learning_rate:\n1e-05\nAttempted to log scalar metric epoch:\n0.1\nAttempted to log scalar metric loss:\n0.6563\nAttempted to log scalar metric grad_norm:\n1.3368971347808838\nAttempted to log scalar metric learning_rate:\n1.5e-05\nAttempted to log scalar metric epoch:\n0.15\nAttempted to log scalar metric loss:\n0.5623\nAttempted to log scalar metric grad_norm:\n1.7434629201889038\nAttempted to log scalar metric learning_rate:\n2e-05\nAttempted to log scalar metric epoch:\n0.2\nAttempted to log scalar metric loss:\n0.4707\nAttempted to log scalar metric grad_norm:\n2.725975275039673\nAttempted to log scalar metric learning_rate:\n2.5e-05\nAttempted to log scalar metric epoch:\n0.25\nAttempted to log scalar metric loss:\n0.3951\nAttempted to log scalar metric grad_norm:\n6.588768482208252\nAttempted to log scalar metric learning_rate:\n3e-05\nAttempted to log scalar metric epoch:\n0.3\nAttempted to log scalar metric loss:\n0.3496\nAttempted to log scalar metric grad_norm:\n14.47787094116211\nAttempted to log scalar metric learning_rate:\n3.5e-05\nAttempted to log scalar metric epoch:\n0.35\nAttempted to log scalar metric loss:\n0.3825\nAttempted to log scalar metric grad_norm:\n8.34833812713623\nAttempted to log scalar metric learning_rate:\n4e-05\nAttempted to log scalar metric epoch:\n0.4\nAttempted to log scalar metric loss:\n0.3886\nAttempted to log scalar metric grad_norm:\n9.559487342834473\nAttempted to log scalar metric learning_rate:\n4.5e-05\nAttempted to log scalar metric epoch:\n0.45\nAttempted to log scalar metric loss:\n0.3087\nAttempted to log scalar metric grad_norm:\n3.5786120891571045\nAttempted to log scalar metric learning_rate:\n5e-05\nAttempted to log scalar metric epoch:\n0.5\nAttempted to log scalar metric loss:\n0.3102\nAttempted to log scalar metric grad_norm:\n8.350960731506348\nAttempted to log scalar metric learning_rate:\n4.899396378269618e-05\nAttempted to log scalar metric epoch:\n0.55\nAttempted to log scalar metric loss:\n0.2901\nAttempted to log scalar metric grad_norm:\n5.929652214050293\nAttempted to log scalar metric learning_rate:\n4.798792756539235e-05\nAttempted to log scalar metric epoch:\n0.6\nAttempted to log scalar metric loss:\n0.3559\nAttempted to log scalar metric grad_norm:\n4.447150707244873\nAttempted to log scalar metric learning_rate:\n4.6981891348088536e-05\nAttempted to log scalar metric epoch:\n0.65\nAttempted to log scalar metric loss:\n0.3908\nAttempted to log scalar metric grad_norm:\n6.023791790008545\nAttempted to log scalar metric learning_rate:\n4.597585513078471e-05\nAttempted to log scalar metric epoch:\n0.7\nAttempted to log scalar metric loss:\n0.4029\nAttempted to log scalar metric grad_norm:\n4.952362537384033\nAttempted to log scalar metric learning_rate:\n4.4969818913480886e-05\nAttempted to log scalar metric epoch:\n0.75\nAttempted to log scalar metric loss:\n0.3537\nAttempted to log scalar metric grad_norm:\n2.5677993297576904\nAttempted to log scalar metric learning_rate:\n4.396378269617706e-05\nAttempted to log scalar metric epoch:\n0.8\nAttempted to log scalar metric loss:\n0.2721\nAttempted to log scalar metric grad_norm:\n5.915972709655762\nAttempted to log scalar metric learning_rate:\n4.295774647887324e-05\nAttempted to log scalar metric epoch:\n0.85\nAttempted to log scalar metric loss:\n0.4204\nAttempted to log scalar metric grad_norm:\n6.89274787902832\nAttempted to log scalar metric learning_rate:\n4.195171026156941e-05\nAttempted to log scalar metric epoch:\n0.9\nAttempted to log scalar metric loss:\n0.3872\nAttempted to log scalar metric grad_norm:\n3.5834689140319824\nAttempted to log scalar metric learning_rate:\n4.0945674044265595e-05\nAttempted to log scalar metric epoch:\n0.95\nAttempted to log scalar metric eval_loss:\n0.3026473820209503\nAttempted to log scalar metric eval_accuracy:\n0.8760226557583386\nAttempted to log scalar metric eval_f1:\n0.9026198714780029\nAttempted to log scalar metric eval_precision:\n0.8942213516160626\nAttempted to log scalar metric eval_recall:\n0.9111776447105788\nAttempted to log scalar metric eval_runtime:\n0.1418\nAttempted to log scalar metric eval_samples_per_second:\n11204.083\nAttempted to log scalar metric eval_steps_per_second:\n176.276\nAttempted to log scalar metric epoch:\n1.0\nAttempted to log scalar metric loss:\n0.3606\nAttempted to log scalar metric grad_norm:\n3.2854108810424805\nAttempted to log scalar metric learning_rate:\n3.993963782696177e-05\nAttempted to log scalar metric epoch:\n1.01\nAttempted to log scalar metric loss:\n0.2867\nAttempted to log scalar metric grad_norm:\n5.3676958084106445\nAttempted to log scalar metric learning_rate:\n3.8933601609657946e-05\nAttempted to log scalar metric epoch:\n1.06\nAttempted to log scalar metric loss:\n0.2599\nAttempted to log scalar metric grad_norm:\n4.839923858642578\nAttempted to log scalar metric learning_rate:\n3.792756539235413e-05\nAttempted to log scalar metric epoch:\n1.11\nAttempted to log scalar metric loss:\n0.2225\nAttempted to log scalar metric grad_norm:\n2.3561408519744873\nAttempted to log scalar metric learning_rate:\n3.6921529175050304e-05\nAttempted to log scalar metric epoch:\n1.16\nAttempted to log scalar metric loss:\n0.2735\nAttempted to log scalar metric grad_norm:\n3.5315701961517334\nAttempted to log scalar metric learning_rate:\n3.5915492957746486e-05\nAttempted to log scalar metric epoch:\n1.21\nAttempted to log scalar metric loss:\n0.2571\nAttempted to log scalar metric grad_norm:\n3.8716444969177246\nAttempted to log scalar metric learning_rate:\n3.490945674044266e-05\nAttempted to log scalar metric epoch:\n1.26\nAttempted to log scalar metric loss:\n0.232\nAttempted to log scalar metric grad_norm:\n2.750293493270874\nAttempted to log scalar metric learning_rate:\n3.390342052313884e-05\nAttempted to log scalar metric epoch:\n1.31\nAttempted to log scalar metric loss:\n0.2156\nAttempted to log scalar metric grad_norm:\n10.314980506896973\nAttempted to log scalar metric learning_rate:\n3.289738430583501e-05\nAttempted to log scalar metric epoch:\n1.36\nAttempted to log scalar metric loss:\n0.1806\nAttempted to log scalar metric grad_norm:\n12.464710235595703\nAttempted to log scalar metric learning_rate:\n3.189134808853119e-05\nAttempted to log scalar metric epoch:\n1.41\nAttempted to log scalar metric loss:\n0.2937\nAttempted to log scalar metric grad_norm:\n6.889758586883545\nAttempted to log scalar metric learning_rate:\n3.088531187122737e-05\nAttempted to log scalar metric epoch:\n1.46\nAttempted to log scalar metric loss:\n0.2241\nAttempted to log scalar metric grad_norm:\n2.17832088470459\nAttempted to log scalar metric learning_rate:\n2.9879275653923545e-05\nAttempted to log scalar metric epoch:\n1.51\nAttempted to log scalar metric loss:\n0.2548\nAttempted to log scalar metric grad_norm:\n7.209200382232666\nAttempted to log scalar metric learning_rate:\n2.887323943661972e-05\nAttempted to log scalar metric epoch:\n1.56\nAttempted to log scalar metric loss:\n0.2539\nAttempted to log scalar metric grad_norm:\n3.5317187309265137\nAttempted to log scalar metric learning_rate:\n2.7867203219315896e-05\nAttempted to log scalar metric epoch:\n1.61\nAttempted to log scalar metric loss:\n0.2111\nAttempted to log scalar metric grad_norm:\n4.864221572875977\nAttempted to log scalar metric learning_rate:\n2.6861167002012072e-05\nAttempted to log scalar metric epoch:\n1.66\nAttempted to log scalar metric loss:\n0.2614\nAttempted to log scalar metric grad_norm:\n2.1613805294036865\nAttempted to log scalar metric learning_rate:\n2.5855130784708247e-05\nAttempted to log scalar metric epoch:\n1.71\nAttempted to log scalar metric loss:\n0.2894\nAttempted to log scalar metric grad_norm:\n2.6568846702575684\nAttempted to log scalar metric learning_rate:\n2.4849094567404426e-05\nAttempted to log scalar metric epoch:\n1.76\nAttempted to log scalar metric loss:\n0.2406\nAttempted to log scalar metric grad_norm:\n4.413516521453857\nAttempted to log scalar metric learning_rate:\n2.3843058350100605e-05\nAttempted to log scalar metric epoch:\n1.81\nAttempted to log scalar metric loss:\n0.2544\nAttempted to log scalar metric grad_norm:\n10.427249908447266\nAttempted to log scalar metric learning_rate:\n2.283702213279678e-05\nAttempted to log scalar metric epoch:\n1.86\nAttempted to log scalar metric loss:\n0.2057\nAttempted to log scalar metric grad_norm:\n8.722635269165039\nAttempted to log scalar metric learning_rate:\n2.1830985915492956e-05\nAttempted to log scalar metric epoch:\n1.91\nAttempted to log scalar metric loss:\n0.1448\nAttempted to log scalar metric grad_norm:\n3.18131422996521\nAttempted to log scalar metric learning_rate:\n2.0824949698189135e-05\nAttempted to log scalar metric epoch:\n1.96\nAttempted to log scalar metric eval_loss:\n0.3618928790092468\nAttempted to log scalar metric eval_accuracy:\n0.8747640025173065\nAttempted to log scalar metric eval_f1:\n0.9015338941118258\nAttempted to log scalar metric eval_precision:\n0.8940137389597644\nAttempted to log scalar metric eval_recall:\n0.909181636726547\nAttempted to log scalar metric eval_runtime:\n0.1393\nAttempted to log scalar metric eval_samples_per_second:\n11405.269\nAttempted to log scalar metric eval_steps_per_second:\n179.441\nAttempted to log scalar metric epoch:\n2.0\nAttempted to log scalar metric loss:\n0.2991\nAttempted to log scalar metric grad_norm:\n2.316530704498291\nAttempted to log scalar metric learning_rate:\n1.9818913480885314e-05\nAttempted to log scalar metric epoch:\n2.01\nAttempted to log scalar metric loss:\n0.1677\nAttempted to log scalar metric grad_norm:\n4.738065719604492\nAttempted to log scalar metric learning_rate:\n1.8812877263581492e-05\nAttempted to log scalar metric epoch:\n2.06\nAttempted to log scalar metric loss:\n0.1973\nAttempted to log scalar metric grad_norm:\n2.981051445007324\nAttempted to log scalar metric learning_rate:\n1.7806841046277668e-05\nAttempted to log scalar metric epoch:\n2.11\nAttempted to log scalar metric loss:\n0.1488\nAttempted to log scalar metric grad_norm:\n2.775860071182251\nAttempted to log scalar metric learning_rate:\n1.6800804828973843e-05\nAttempted to log scalar metric epoch:\n2.16\nAttempted to log scalar metric loss:\n0.1142\nAttempted to log scalar metric grad_norm:\n5.452521324157715\nAttempted to log scalar metric learning_rate:\n1.5794768611670022e-05\nAttempted to log scalar metric epoch:\n2.21\nAttempted to log scalar metric loss:\n0.1885\nAttempted to log scalar metric grad_norm:\n6.432653427124023\nAttempted to log scalar metric learning_rate:\n1.4788732394366198e-05\nAttempted to log scalar metric epoch:\n2.26\nAttempted to log scalar metric loss:\n0.1562\nAttempted to log scalar metric grad_norm:\n2.9526960849761963\nAttempted to log scalar metric learning_rate:\n1.3782696177062373e-05\nAttempted to log scalar metric epoch:\n2.31\nAttempted to log scalar metric loss:\n0.1834\nAttempted to log scalar metric grad_norm:\n2.500070571899414\nAttempted to log scalar metric learning_rate:\n1.2776659959758552e-05\nAttempted to log scalar metric epoch:\n2.36\nAttempted to log scalar metric loss:\n0.1538\nAttempted to log scalar metric grad_norm:\n1.4474155902862549\nAttempted to log scalar metric learning_rate:\n1.177062374245473e-05\nAttempted to log scalar metric epoch:\n2.41\nAttempted to log scalar metric loss:\n0.1701\nAttempted to log scalar metric grad_norm:\n3.299621820449829\nAttempted to log scalar metric learning_rate:\n1.0764587525150906e-05\nAttempted to log scalar metric epoch:\n2.46\nAttempted to log scalar metric loss:\n0.1481\nAttempted to log scalar metric grad_norm:\n12.052362442016602\nAttempted to log scalar metric learning_rate:\n9.758551307847083e-06\nAttempted to log scalar metric epoch:\n2.51\nAttempted to log scalar metric loss:\n0.1845\nAttempted to log scalar metric grad_norm:\n3.21598744392395\nAttempted to log scalar metric learning_rate:\n8.75251509054326e-06\nAttempted to log scalar metric epoch:\n2.56\nAttempted to log scalar metric loss:\n0.1764\nAttempted to log scalar metric grad_norm:\n3.642615795135498\nAttempted to log scalar metric learning_rate:\n7.746478873239436e-06\nAttempted to log scalar metric epoch:\n2.61\nAttempted to log scalar metric loss:\n0.2255\nAttempted to log scalar metric grad_norm:\n1.8091888427734375\nAttempted to log scalar metric learning_rate:\n6.740442655935614e-06\nAttempted to log scalar metric epoch:\n2.66\nAttempted to log scalar metric loss:\n0.1538\nAttempted to log scalar metric grad_norm:\n1.9823811054229736\nAttempted to log scalar metric learning_rate:\n5.734406438631791e-06\nAttempted to log scalar metric epoch:\n2.71\nAttempted to log scalar metric loss:\n0.1493\nAttempted to log scalar metric grad_norm:\n2.713505744934082\nAttempted to log scalar metric learning_rate:\n4.7283702213279675e-06\nAttempted to log scalar metric epoch:\n2.76\nAttempted to log scalar metric loss:\n0.1128\nAttempted to log scalar metric grad_norm:\n1.6512892246246338\nAttempted to log scalar metric learning_rate:\n3.722334004024145e-06\nAttempted to log scalar metric epoch:\n2.81\nAttempted to log scalar metric loss:\n0.171\nAttempted to log scalar metric grad_norm:\n5.502684593200684\nAttempted to log scalar metric learning_rate:\n2.7162977867203223e-06\nAttempted to log scalar metric epoch:\n2.86\nAttempted to log scalar metric loss:\n0.1195\nAttempted to log scalar metric grad_norm:\n3.3188114166259766\nAttempted to log scalar metric learning_rate:\n1.7102615694164992e-06\nAttempted to log scalar metric epoch:\n2.91\nAttempted to log scalar metric loss:\n0.1616\nAttempted to log scalar metric grad_norm:\n3.978597640991211\nAttempted to log scalar metric learning_rate:\n7.042253521126761e-07\nAttempted to log scalar metric epoch:\n2.96\nAttempted to log scalar metric eval_loss:\n0.36574485898017883\nAttempted to log scalar metric eval_accuracy:\n0.8703587161736942\nAttempted to log scalar metric eval_f1:\n0.8967935871743486\nAttempted to log scalar metric eval_precision:\n0.9004024144869215\nAttempted to log scalar metric eval_recall:\n0.8932135728542914\nAttempted to log scalar metric eval_runtime:\n0.1323\nAttempted to log scalar metric eval_samples_per_second:\n12009.033\nAttempted to log scalar metric eval_steps_per_second:\n188.94\nAttempted to log scalar metric epoch:\n3.0\nAttempted to log scalar metric train_runtime:\n27.201\nAttempted to log scalar metric train_samples_per_second:\n700.892\nAttempted to log scalar metric train_steps_per_second:\n21.948\nAttempted to log scalar metric total_flos:\n34528196655540.0\nAttempted to log scalar metric train_loss:\n0.2764041194164973\nAttempted to log scalar metric epoch:\n3.0\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 38,
          "data": {
            "text/plain": "TrainOutput(global_step=597, training_loss=0.2764041194164973, metrics={'train_runtime': 27.201, 'train_samples_per_second': 700.892, 'train_steps_per_second': 21.948, 'total_flos': 34528196655540.0, 'train_loss': 0.2764041194164973, 'epoch': 3.0})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 38,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714107126456
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def print_evaluation_results(trainer):\n",
        "    # Evaluate the model and store the results\n",
        "    evaluation_results = trainer.evaluate()\n",
        "\n",
        "    # Print evaluation results\n",
        "    for metric_name, metric_value in evaluation_results.items():\n",
        "        print(f\"{metric_name}: {metric_value}\")\n",
        "\n",
        "# Execute the function to print evaluation results\n",
        "print_evaluation_results(trainer)\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [25/25 00:00]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Attempted to log scalar metric eval_loss:\n0.36574485898017883\nAttempted to log scalar metric eval_accuracy:\n0.8703587161736942\nAttempted to log scalar metric eval_f1:\n0.8967935871743486\nAttempted to log scalar metric eval_precision:\n0.9004024144869215\nAttempted to log scalar metric eval_recall:\n0.8932135728542914\nAttempted to log scalar metric eval_runtime:\n0.1566\nAttempted to log scalar metric eval_samples_per_second:\n10148.725\nAttempted to log scalar metric eval_steps_per_second:\n159.672\nAttempted to log scalar metric epoch:\n3.0\neval_loss: 0.36574485898017883\neval_accuracy: 0.8703587161736942\neval_f1: 0.8967935871743486\neval_precision: 0.9004024144869215\neval_recall: 0.8932135728542914\neval_runtime: 0.1566\neval_samples_per_second: 10148.725\neval_steps_per_second: 159.672\nepoch: 3.0\n"
        }
      ],
      "execution_count": 39,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714107126644
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lets create predictions"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_name(name):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Tokenize the input name\n",
        "    encodings = tokenizer(name, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "    \n",
        "    encodings = {key: val.to(device) for key, val in encodings.items()}\n",
        "\n",
        "    # Predict using the trained model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encodings)\n",
        "\n",
        "    # Get the prediction probabilities and apply softmax\n",
        "    logits = outputs.logits\n",
        "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "    # Extract probabilities for each class\n",
        "    male_prob, female_prob = probs[0, 0].item(), probs[0, 1].item()\n",
        "\n",
        "    # Determine predicted class based on the higher probability\n",
        "    predicted_class = 'Male' if male_prob > female_prob else 'Female'\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Name: {name}\")\n",
        "    print(f\"Predicted Gender: {predicted_class}\")\n",
        "    print(f\"Probability (Male): {male_prob:.4f}\")\n",
        "    print(f\"Probability (Female): {female_prob:.4f}\")\n",
        "\n",
        "\n",
        "predict_name(\"Alex\")\n",
        "predict_name(\"Emma\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Name: Alex\nPredicted Gender: Male\nProbability (Male): 0.6109\nProbability (Female): 0.3891\nName: Emma\nPredicted Gender: Female\nProbability (Male): 0.0096\nProbability (Female): 0.9904\n"
        }
      ],
      "execution_count": 40,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714107195223
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM based approach"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import names\n",
        "\n",
        "\n",
        "nltk.download('names')\n",
        "\n",
        "def preprocess_names_data(male_names, female_names):\n",
        "    names = male_names + female_names\n",
        "    names = [str(name) for name in names if isinstance(name, (str, tuple))]\n",
        "    return names\n",
        "\n",
        "def tokenize_and_pad(names):\n",
        "    tokenizer = Tokenizer(char_level=True)\n",
        "    tokenizer.fit_on_texts(names)\n",
        "    sequences = tokenizer.texts_to_sequences(names)\n",
        "    data = pad_sequences(sequences, maxlen=10)\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    return data, vocab_size\n",
        "\n",
        "def prepare_data(names_data, labels):\n",
        "    train_data, val_data, train_labels, val_labels = train_test_split(names_data, labels, test_size=0.2, random_state=42)\n",
        "    return train_data, val_data, train_labels, val_labels\n",
        "\n",
        "def create_loaders(train_data, val_data, train_labels, val_labels, batch_size=32):\n",
        "    train_dataset = NamesDataset(train_data, train_labels)\n",
        "    val_dataset = NamesDataset(val_data, val_labels)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def train_model(model, train_loader, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_preds = 0\n",
        "    for x, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        loss = nn.BCEWithLogitsLoss()(output.squeeze(), y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        predictions = torch.round(torch.sigmoid(output)).squeeze()\n",
        "        correct_preds += (predictions == y).sum().item()\n",
        "    accuracy = correct_preds / len(train_loader.dataset)\n",
        "    return total_loss / len(train_loader), accuracy\n",
        "\n",
        "def validate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct_preds = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            output = model(x)\n",
        "            loss = nn.BCEWithLogitsLoss()(output.squeeze(), y)\n",
        "            total_loss += loss.item()\n",
        "            predictions = torch.round(torch.sigmoid(output)).squeeze()\n",
        "            correct_preds += (predictions == y).sum().item()\n",
        "    accuracy = correct_preds / len(val_loader.dataset)\n",
        "    return total_loss / len(val_loader), accuracy\n",
        "\n",
        "def train_and_validate(model, train_loader, val_loader, optimizer, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = train_model(model, train_loader, optimizer)\n",
        "        val_loss, val_acc = validate_model(model, val_loader)\n",
        "        print(f\"Epoch: {epoch+1}, Training Loss: {train_loss}, Training Accuracy: {train_acc}, Validation Loss: {val_loss}, Validation Accuracy: {val_acc}\")\n",
        "\n",
        "# Custom Dataset class\n",
        "class NamesDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = torch.tensor(data, dtype=torch.long)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.float)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index], self.labels[index]\n",
        "\n",
        "# Model definition\n",
        "class NameClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(NameClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc(x[:, -1, :])\n",
        "        return x\n",
        "\n",
        "\n",
        "def main(male_names, female_names):\n",
        "    # Preprocess data\n",
        "    names = preprocess_names_data(male_names, female_names)\n",
        "    labels = np.array([0]*len(male_names) + [1]*len(female_names))\n",
        "\n",
        "    # Tokenize and pad data\n",
        "    data, vocab_size = tokenize_and_pad(names)\n",
        "\n",
        "    # Prepare data\n",
        "    train_data, val_data, train_labels, val_labels = prepare_data(data, labels)\n",
        "\n",
        "    # Create loaders\n",
        "    train_loader, val_loader = create_loaders(train_data, val_data, train_labels, val_labels)\n",
        "\n",
        "    # Initialize model\n",
        "    model = NameClassifier(vocab_size, embedding_dim=50, hidden_dim=100)\n",
        "    optimizer = Adam(model.parameters())\n",
        "\n",
        "    # Train and validate model\n",
        "    train_and_validate(model, train_loader, val_loader, optimizer)\n",
        "\n",
        "male_names = [(name, 0) for name in names.words('male.txt')]\n",
        "female_names = [(name, 1) for name in names.words('female.txt')]\n",
        "main(male_names, female_names)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[nltk_data] Downloading package names to /home/azureuser/nltk_data...\n[nltk_data]   Package names is already up-to-date!\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch: 1, Training Loss: 0.05401957650252398, Training Accuracy: 0.9776553894571204, Validation Loss: 0.0006162651634076611, Validation Accuracy: 1.0\nEpoch: 2, Training Loss: 0.0003651852564314081, Training Accuracy: 1.0, Validation Loss: 0.00022718803375028075, Validation Accuracy: 1.0\nEpoch: 3, Training Loss: 0.00016512479491229988, Training Accuracy: 1.0, Validation Loss: 0.00012463565944926814, Validation Accuracy: 1.0\nEpoch: 4, Training Loss: 9.860007105372968e-05, Training Accuracy: 1.0, Validation Loss: 8.06317522074096e-05, Validation Accuracy: 1.0\nEpoch: 5, Training Loss: 6.669550723803987e-05, Training Accuracy: 1.0, Validation Loss: 5.694219456927385e-05, Validation Accuracy: 1.0\nEpoch: 6, Training Loss: 4.842155949381091e-05, Training Accuracy: 1.0, Validation Loss: 4.2483297002036124e-05, Validation Accuracy: 1.0\nEpoch: 7, Training Loss: 3.679467561450289e-05, Training Accuracy: 1.0, Validation Loss: 3.288896754384041e-05, Validation Accuracy: 1.0\nEpoch: 8, Training Loss: 2.8872925940045182e-05, Training Accuracy: 1.0, Validation Loss: 2.616136636788724e-05, Validation Accuracy: 1.0\nEpoch: 9, Training Loss: 2.319535503635445e-05, Training Accuracy: 1.0, Validation Loss: 2.1225955460977275e-05, Validation Accuracy: 1.0\nEpoch: 10, Training Loss: 1.897818275032496e-05, Training Accuracy: 1.0, Validation Loss: 1.7512341073597782e-05, Validation Accuracy: 1.0\n"
        }
      ],
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714106962181
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lets generate inferences"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_input_names(input_names, tokenizer):\n",
        "    sequences = tokenizer.texts_to_sequences(input_names)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=10)\n",
        "    return padded_sequences\n",
        "\n",
        "\n",
        "def generate_inferences(model, input_names, tokenizer):\n",
        "    # Preprocess input names\n",
        "    input_data = preprocess_input_names(input_names, tokenizer)\n",
        "    \n",
        "    # Convert input data to PyTorch tensor\n",
        "    input_tensor = torch.tensor(input_data, dtype=torch.long)\n",
        "    \n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        predictions = torch.round(torch.sigmoid(outputs)).squeeze().numpy()\n",
        "    \n",
        "    # Convert predictions to gender labels\n",
        "    gender_labels = [\"Male\" if pred == 0 else \"Female\" for pred in predictions]\n",
        "    \n",
        "    return list(zip(input_names, gender_labels))\n",
        "\n",
        "\n",
        "input_names = [\"John\", \"Emma\", \"Michael\"]\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(input_names)\n",
        "\n",
        "\n",
        "gender_names = generate_inferences(model, input_names, tokenizer)\n",
        "for name, gender in gender_names:\n",
        "    print(f\"Name: {name}, Gender: {gender}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Name: John, Gender: Male\nName: Emma, Gender: Female\nName: Michael, Gender: Male\n"
        }
      ],
      "execution_count": 44,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1714107363601
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "1. https://arxiv.org/pdf/2102.03692.pdf\n",
        "2. https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/exercise/13-attention.html\n",
        "3. https://towardsdatascience.com/deep-learning-gender-from-name-lstm-recurrent-neural-networks-448d64553044\n",
        "4. https://www.nltk.org/book/ch02.html#sec-lexical-resources"
      ],
      "metadata": {
        "id": "ExMITGgCdQWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PHiDsdXLhbbW"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "m3_assignment_part_III.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNfOtlff/PWjuBUckXySCDS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "myenv",
      "language": "python",
      "display_name": "Python (myenv)"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "myenv"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}